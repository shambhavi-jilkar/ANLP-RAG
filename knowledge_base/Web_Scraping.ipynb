{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to mimic a real browser request\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "BASE_URLS = {\n",
    "    # \"https://www.visitpittsburgh.com/things-to-do/\":{'category': 'Pittsburgh', 'subcategory':'Events'},\n",
    "    # \"https://pittsburghrestaurantweek.com/\": {'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.pghtacofest.com/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://littleitalydays.com/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.picklesburgh.com/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},   \n",
    "    # \"https://pittsburghsymphony.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.pghtech.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.thefrickpittsburgh.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/blog/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://trustarts.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/events-festivals/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/hotels-resorts/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/restaurants-culinary/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Cork_Run_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Fort_Pitt_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Liberty_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Mount_Washington_Transit_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Squirrel_Hill_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Schenley_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/List_of_bridges_of_Pittsburgh\":{'category': 'Pittsburgh', 'subcategory': 'Facts'}\n",
    "    \"https://www.heinzhistorycenter.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'}\n",
    "    \n",
    "}\n",
    "\n",
    "# Output files\n",
    "OUTPUT_DIRECTORY = os.path.join(os.getcwd(), \"json_documents\")\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok = True)\n",
    "ERROR_LOG = \"scraping_errors.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_function(soup):\n",
    "    event_cards = soup.find_all(\"li\", class_=\"events_list_item\")\n",
    "    \n",
    "    content_list = []\n",
    "    print(len(event_cards))\n",
    "\n",
    "    for card in event_cards:\n",
    "        try:\n",
    "            title = card.find(\"span\", class_=\"card_title_link_inner\").text.strip()\n",
    "            date = card.find(\"p\", class_=\"card_date\").text.strip()\n",
    "            time = card.find(\"span\", class_=\"card_time\").text.strip()\n",
    "            location = card.find(\"span\", class_=\"card_location\").text.strip()\n",
    "            description = card.find(\"div\", class_=\"card_description\").p.text.strip()\n",
    "            #         start_date = card.get('data-event-start')\n",
    "    #         end_date = card.get('data-event-end')\n",
    "    \n",
    "    #         recurrence_event = card.get('data-recurring-event', False)\n",
    "    #         recurrence_frequency = card.get('data-recurring-frequency', None)\n",
    "    #         weekday = card.get('data-day-weekday', None)\n",
    "    \n",
    "    #         title_element = card.find('div', class_='event-card__content')\n",
    "    #         title = title_element.text.strip() if title_element else \"No title\"\n",
    "    #         link = None\n",
    "    #         if title_element and title_element.find('a'):\n",
    "    #             link = title_element.find('a').get('href')\n",
    "            \n",
    "    #         venue_element = card.find('div', class_='event-card__venue')\n",
    "    #         venue = None\n",
    "    #         if venue_element and venue_element.find('a'):\n",
    "    #             venue = venue_element.find('a').text.strip()\n",
    "            \n",
    "    #         # Extract details\n",
    "    #         details_element = card.find('div', class_='event-card__details')\n",
    "    #         details = details_element.text.strip() if details_element else None\n",
    "            \n",
    "            content = f\"{title} is on {date} at {time}. It will be located at {location}. {description}\"\n",
    "            \n",
    "    #         if recurrence_event:\n",
    "                # content += f\"The event occurs on {recurrence_frequency} on {weekday}\"\n",
    "            \n",
    "            content_list.append(content)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing event card: {e}\")\n",
    "            continue\n",
    "        \n",
    "    return \"\\n\".join(content_list)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_base_domain(url):\n",
    "#     pattern = r'^(?:https?:\\/\\/)?(?:www\\.)?([^\\/]+?)\\.(?:com|org|gov|edu)(?:\\/|$)'\n",
    "#     match = re.search(pattern, url)\n",
    "#     return match.group(1) if match else None\n",
    "\n",
    "# Function to scrape a single webpage\n",
    "def scrape_page(url, depth, max_depth, category, subcategory, scraped_urls):\n",
    "    \"\"\"Scrapes headings and paragraphs from a given webpage.\"\"\"\n",
    "    if depth > max_depth:\n",
    "        return None \n",
    "\n",
    "    response = requests.get(url, headers=HEADERS, timeout=10)  # 10s timeout\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Skipping {url} (Status Code: {response.status_code})\")\n",
    "        with open(ERROR_LOG, \"a\", encoding=\"utf-8\") as err_file:\n",
    "            err_file.write(f\"Failed: {url} (Status Code: {response.status_code})\\n\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    if url not in scraped_urls:\n",
    "        print(f\"Scraping (Depth {depth}): {url}\")\n",
    "        title_tag = soup.find(\"title\")\n",
    "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
    "        title = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1F]', '', title)\n",
    "\n",
    "        content_list = [f\"Title: {title}\\nURL: {url}\\n\"]\n",
    "        content = \"\"\n",
    "        # Extract headings & paragraphs\n",
    "        for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"]):\n",
    "            if tag.name in [\"h1\", \"h2\", \"h3\"]:  # Headings\n",
    "                content_list.append(f\"\\n{tag.text.strip()}\\n\" + \"-\" * len(tag.text.strip()) + \"\\n\")\n",
    "            elif tag.name == \"p\":  # Paragraphs\n",
    "                text = tag.get_text().strip()\n",
    "                if text:\n",
    "                    content_list.append(text)\n",
    "                    content = \"\\n\".join(content_list)\n",
    "                    \n",
    "        if \"www.heinzhistorycenter.org/events\" in url:\n",
    "            content = events_function(soup)\n",
    "        \n",
    "        max_id = max(\n",
    "            [0] + [\n",
    "                int(s.split(\"_\")[0])\n",
    "                for s in os.listdir(OUTPUT_DIRECTORY)\n",
    "                if s.endswith('.json')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if content != \"\": \n",
    "            document = {\n",
    "                    \"content\": content,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": url,\n",
    "                        \"title\": title,\n",
    "                        \"category\": category,\n",
    "                        \"subcategory\": subcategory,\n",
    "                        \"date_scraped\": datetime.datetime.now().isoformat(),\n",
    "                        \"depth\": depth,\n",
    "                        'id':max_id + 1\n",
    "                    }\n",
    "                }\n",
    "            doc_id = document[\"metadata\"][\"id\"]\n",
    "            \n",
    "            filename = f\"{doc_id}_{title}.json\"\n",
    "            filepath = os.path.join(OUTPUT_DIRECTORY, filename)\n",
    "\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(document, f, ensure_ascii=False, indent=2)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def recursive_scrape(url, depth, category, subcategory, visited_urls, scraped_urls, max_depth = 3, base_url=None):\n",
    "    print(url)\n",
    "    if depth > max_depth or url == \"https://carnegiemuseums.org/\" or url == \"https://carnegiemuseums.org\":\n",
    "        return\n",
    "    \n",
    "    if not base_url:\n",
    "        # parsed_url = urllib.parse.urlparse(url)\n",
    "        # base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        base_url = url\n",
    "    \n",
    "    visited_urls.add(url)\n",
    "    \n",
    "    soup = None\n",
    "    soup = scrape_page(url, depth, max_depth, category, subcategory, scraped_urls)\n",
    "    scraped_urls.add(url)\n",
    "    if not soup:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    if max_depth > 0 and depth < max_depth:\n",
    "        links = []\n",
    "        \n",
    "        if \"en.wikipedia\" in url:\n",
    "            tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "            links = []\n",
    "            for table in tables:\n",
    "                for row in table.find_all(\"tr\")[1:]: \n",
    "                    if row and len(row) > 0:\n",
    "                        link_tags = row.find_all(\"a\", href=True)\n",
    "                        if link_tags and link_tags[0][\"href\"].startswith(\"/wiki/\"):\n",
    "                            link = link_tags[0]\n",
    "                            links.append(\"https://en.wikipedia.org\" + link[\"href\"])\n",
    "            \n",
    "\n",
    "        else:\n",
    "            full_url = None\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                href = link[\"href\"]\n",
    "                full_url = None\n",
    "\n",
    "                if href.startswith(\"/\"):\n",
    "                    full_url = urllib.parse.urljoin(base_url, href)  # Use urljoin to correctly resolve relative paths\n",
    "                elif href.startswith((\"http://\", \"https://\")):\n",
    "                    parsed_href = urllib.parse.urlparse(href)\n",
    "                    parsed_base = urllib.parse.urlparse(base_url)\n",
    "\n",
    "                    # Ensure the link belongs to the same domain\n",
    "                    if parsed_href.netloc == parsed_base.netloc:\n",
    "                        full_url = href\n",
    "\n",
    "                if full_url and full_url not in visited_urls:\n",
    "                    links.append(full_url)\n",
    "        \n",
    "        print(f\"Found {len(links)} links at {url}\")\n",
    "        # print(links)\n",
    "        for link in links:\n",
    "            if link not in visited_urls:\n",
    "                time.sleep(2)\n",
    "                recursive_scrape(link, depth+1, category, subcategory, visited_urls, scraped_urls, max_depth) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scraped_urls(documents_dir):\n",
    "    scraped_urls = set()\n",
    "    \n",
    "    for filename in os.listdir(documents_dir):\n",
    "        if not filename.endswith('.json'):\n",
    "            continue\n",
    "        filepath = os.path.join(documents_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            document = json.load(f)\n",
    "            if 'source' in document['metadata']:\n",
    "                url = document['metadata']['source']\n",
    "                scraped_urls.add(url)\n",
    "    \n",
    "    return scraped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.heinzhistorycenter.org/\n",
      "Found 105 links at https://www.heinzhistorycenter.org/\n",
      "https://www.heinzhistorycenter.org/whats-on/history-center/\n",
      "Found 102 links at https://www.heinzhistorycenter.org/whats-on/history-center/\n",
      "https://www.heinzhistorycenter.org/whats-on/sports-museum/\n",
      "Found 96 links at https://www.heinzhistorycenter.org/whats-on/sports-museum/\n",
      "https://www.heinzhistorycenter.org/whats-on/fort-pitt/\n",
      "Found 83 links at https://www.heinzhistorycenter.org/whats-on/fort-pitt/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/\n",
      "Found 90 links at https://www.heinzhistorycenter.org/whats-on/meadowcroft/\n",
      "https://www.heinzhistorycenter.org/join/\n",
      "https://www.heinzhistorycenter.org/give/make-a-donation/\n",
      "https://www.heinzhistorycenter.org/search/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/rockshelter/\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/rockshelter/\n",
      "https://www.heinzhistorycenter.org/whats-on/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/events/\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/whats-on/meadowcroft/events/\n",
      "https://www.heinzhistorycenter.org/visit/meadowcroft/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/historic-village/\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/historic-village/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/indian-village/\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/indian-village/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/frontier-trading-post/\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/frontier-trading-post/\n",
      "https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/trails-to-trains/\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/whats-on/meadowcroft/exhibits/trails-to-trains/\n",
      "https://www.heinzhistorycenter.org/about/\n",
      "https://www.heinzhistorycenter.org/about/contact-us/\n",
      "https://www.heinzhistorycenter.org/visit/accessibility/\n",
      "https://www.heinzhistorycenter.org/visit/health-safety/\n",
      "https://www.heinzhistorycenter.org/about/work-with-us/\n",
      "https://www.heinzhistorycenter.org/sitemap/\n",
      "https://www.heinzhistorycenter.org/policies/\n",
      "https://www.google.com/maps/place/?q=place_id:ChIJr7aBzePzNIgRi2Dp2ZCFmUY\n",
      "Scraping (Depth 5): https://www.google.com/maps/place/?q=place_id:ChIJr7aBzePzNIgRi2Dp2ZCFmUY\n",
      "https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#88e1e6eee7c8e0ede1e6f2e0e1fbfce7faf1ebede6fcedfaa6e7faef\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#88e1e6eee7c8e0ede1e6f2e0e1fbfce7faf1ebede6fcedfaa6e7faef\n",
      "https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#e0898e868fa08885898e9a888993948f929983858e948592ce8f9287\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#e0898e868fa08885898e9a888993948f929983858e948592ce8f9287\n",
      "https://www.google.com/maps/place/?q=place_id:ChIJuwu6IVXxNIgRa1BuX7bHZ1Q\n",
      "Scraping (Depth 5): https://www.google.com/maps/place/?q=place_id:ChIJuwu6IVXxNIgRa1BuX7bHZ1Q\n",
      "https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#096f667b7d79607d7d60676f6649616c60677361607a7d667b706a6c677d6c7b27667b6e\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#096f667b7d79607d7d60676f6649616c60677361607a7d667b706a6c677d6c7b27667b6e\n",
      "https://www.google.com/maps/place/?q=place_id:ChIJ_____z81NIgRAIsIZOTiR-A\n",
      "Scraping (Depth 5): https://www.google.com/maps/place/?q=place_id:ChIJ_____z81NIgRAIsIZOTiR-A\n",
      "https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#f9949c989d968e9a8b969f8d90979f96b9919c90978391908a8d968b809a9c978d9c8bd7968b9e\n",
      "Scraping (Depth 5): https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#f9949c989d968e9a8b969f8d90979f96b9919c90978391908a8d968b809a9c978d9c8bd7968b9e\n"
     ]
    }
   ],
   "source": [
    "# Visited URLs (to prevent duplicate scraping)\n",
    "scraped_urls = get_scraped_urls(OUTPUT_DIRECTORY)\n",
    "visited_urls = set()\n",
    "\n",
    "for url in BASE_URLS:\n",
    "    recursive_scrape(url, \n",
    "                     depth=0, \n",
    "                     category = BASE_URLS[url]['category'], \n",
    "                     subcategory =BASE_URLS[url]['subcategory'], \n",
    "                     visited_urls = visited_urls, \n",
    "                     scraped_urls=scraped_urls, \n",
    "                     max_depth=5, \n",
    "                     base_url=url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1815_Email Protection  Cloudflare.json\n",
      "1816_Email Protection  Cloudflare.json\n",
      "1817_Email Protection  Cloudflare.json\n",
      "1818_Email Protection  Cloudflare.json\n",
      "1820_Email Protection  Cloudflare.json\n",
      "1821_Email Protection  Cloudflare.json\n",
      "1822_Email Protection  Cloudflare.json\n",
      "1823_Email Protection  Cloudflare.json\n",
      "1824_Email Protection  Cloudflare.json\n",
      "1825_Email Protection  Cloudflare.json\n",
      "1826_Email Protection  Cloudflare.json\n",
      "1827_Email Protection  Cloudflare.json\n",
      "1828_Email Protection  Cloudflare.json\n",
      "1829_Email Protection  Cloudflare.json\n",
      "1830_Email Protection  Cloudflare.json\n",
      "1831_Email Protection  Cloudflare.json\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# SKIP_PATTERN = re.compile(r\"(_Terms|_Notice|_Become|_Privacy)\")\n",
    "# EXCLUSIONS = ['_Email Protection']\n",
    "# for filename in os.listdir(OUTPUT_DIRECTORY):\n",
    "#     count = 0\n",
    "#     if any(x in filename for x in EXCLUSIONS): \n",
    "#         count+=1\n",
    "#         print(filename)\n",
    "#         os.remove(os.path.join(OUTPUT_DIRECTORY, filename))\n",
    "# print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
