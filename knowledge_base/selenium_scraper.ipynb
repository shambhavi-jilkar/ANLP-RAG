{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pittsburgh Events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "def get_driver(headless=False):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def click_show_more_buttons(driver):\n",
    "    clicked = False \n",
    "    try:\n",
    "        show_more_gsb = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//div[contains(@class, 'gsb-show-more')]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", show_more_gsb)\n",
    "        time.sleep(2)\n",
    "        clicked = True\n",
    "    except:\n",
    "        pass  \n",
    "\n",
    "    try:\n",
    "        show_more_ptb = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//div[contains(@class, 'ptb-show-more')]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", show_more_ptb)\n",
    "        time.sleep(2)\n",
    "        clicked = True\n",
    "    except:\n",
    "        pass  \n",
    "\n",
    "    return clicked \n",
    "\n",
    "\n",
    "def scrape_events_page(url):\n",
    "    driver = get_driver(headless=False)\n",
    "    driver.get(url)\n",
    "\n",
    "    while True:\n",
    "        clicked = click_show_more_buttons(driver)\n",
    "        if not clicked:\n",
    "            print(\"No more 'Show More' All events should be loaded.\")\n",
    "            break\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    events = []\n",
    "    date_rows = soup.find_all(\"li\", class_=\"date-row\")\n",
    "    print(\"Found\", len(date_rows), \"date-row events.\")\n",
    "    for row in date_rows:\n",
    "        try:\n",
    "            name_tag = row.find(\"div\", class_=\"venue\")\n",
    "            name_link = name_tag.find(\"a\") if name_tag else None\n",
    "\n",
    "            date_block  = row.find(\"div\", class_=\"date\")\n",
    "            time_block  = row.find(\"div\", class_=\"time\")\n",
    "            venue_block = row.find(\"div\", class_=\"date-desc\")\n",
    "            price_block = row.find(\"div\", class_=\"from-price\")\n",
    "\n",
    "            name = clean_text(name_link.get_text()) if name_link else \"No name\"\n",
    "            date_text = clean_text(date_block.get_text()) if date_block else \"\"\n",
    "            time_text = clean_text(time_block.get_text()) if time_block else \"\"\n",
    "            venue_text = clean_text(venue_block.get_text()) if venue_block else \"\"\n",
    "            price_text = clean_text(price_block.get_text()) if price_block else \"\"\n",
    "\n",
    "            event_str = f\"{name} | {date_text} | {time_text} | {venue_text} | {price_text}\"\n",
    "            events.append(event_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping one date-row event due to error: {e}\")\n",
    "            continue\n",
    "   \n",
    "    char_listings = soup.find_all(\"a\", class_=\"v-list-char\")\n",
    "    print(\"Found\", len(char_listings), \"v-list-char events.\")\n",
    "    for link in char_listings:\n",
    "        try:\n",
    "            pname_div = link.find(\"div\", class_=\"pname\")\n",
    "            name = clean_text(pname_div.get_text()) if pname_div else \"No name\"\n",
    "\n",
    "            pcount_div = link.find(\"div\", class_=\"pcount\")\n",
    "            pcount_text = clean_text(pcount_div.get_text()) if pcount_div else \"\"\n",
    "\n",
    "            event_str = f\"{name} | {pcount_text}\"\n",
    "            events.append(event_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping one v-list-char event due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Scraped {len(events)} events.\")\n",
    "\n",
    "    return \"\\n\".join(events)\n",
    "\n",
    "def main():\n",
    "    # URLs for each month\n",
    "    urls_by_month = {\n",
    "        \"march\":      \"https://pittsburgh.events/march/\",\n",
    "        \"april\":      \"https://pittsburgh.events/april/\",\n",
    "        \"may\":        \"https://pittsburgh.events/may/\",\n",
    "        \"june\":       \"https://pittsburgh.events/june/\",\n",
    "        \"july\":       \"https://pittsburgh.events/july/\",\n",
    "        \"august\":     \"https://pittsburgh.events/august/\",\n",
    "        \"september\":  \"https://pittsburgh.events/september/\",\n",
    "        \"october\":    \"https://pittsburgh.events/october/\",\n",
    "        \"november\":   \"https://pittsburgh.events/november/\",\n",
    "        \"december\":   \"https://pittsburgh.events/december/\"\n",
    "    }\n",
    "\n",
    "    doc_id = 1\n",
    "\n",
    "    for month_name, url in urls_by_month.items():\n",
    "        print(f\"Scraping {month_name.title()} => {url}\")\n",
    "        events_data = scrape_events_page(url)\n",
    "        doc = {\n",
    "            \"content\": events_data, \n",
    "            \"metadata\": {\n",
    "                \"source\": url,\n",
    "                \"title\": f\"Pittsburgh Events - {month_name.title()}\",\n",
    "                \"date_scraped\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        filename = f\"pittsburgh_events_{month_name}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Saved {filename}\")\n",
    "        doc_id += 1\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pittsburgh Downtown Events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Removes extra spaces, tabs, and newlines, making content clean and readable. \"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "def get_driver(headless=True):\n",
    "    \"\"\" Configure Selenium WebDriver with Chrome in headless mode. \"\"\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def scrape_downtown_pittsburgh_events():\n",
    "    \"\"\" Scrapes events from Downtown Pittsburgh Events page and returns a formatted string. \"\"\"\n",
    "    url = \"https://downtownpittsburgh.com/events/\"\n",
    "    driver = get_driver(headless=True)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  \n",
    "\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    events = []\n",
    "    event_items = soup.find_all(\"div\", class_=\"copyContent\")\n",
    "\n",
    "    for event in event_items:\n",
    "        try:\n",
    "            category_div = event.find(\"div\", class_=\"term\")\n",
    "            category = clean_text(category_div.get_text()) if category_div else \"No Category\"\n",
    "            title_h1 = event.find(\"h1\")\n",
    "            title_link = title_h1.find(\"a\") if title_h1 else None\n",
    "            title = clean_text(title_link.get_text()) if title_link else \"No Title\"\n",
    "            date_div = event.find(\"div\", class_=\"eventdate\")\n",
    "            date_time = clean_text(date_div.get_text()) if date_div else \"No Date/Time\"\n",
    "            event_description = event.get_text().replace(\"READ MORE\", \"\").strip()\n",
    "            event_description = clean_text(event_description.replace(date_time, \"\").replace(title, \"\").replace(category, \"\"))\n",
    "            event_str = f\"{category} | {title} | {date_time} | {event_description}\"\n",
    "            events.append(event_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping an event due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Scraped {len(events)} events.\")\n",
    "    return \" --- \".join(events)\n",
    "\n",
    "def main():\n",
    "    \"\"\" Scrapes events and saves them in JSON format with a clean string. \"\"\"\n",
    "    event_text = scrape_downtown_pittsburgh_events()  # Get cleaned event text\n",
    "    doc = {\n",
    "        \"content\": event_text, \n",
    "        \"metadata\": {\n",
    "            \"source\": \"https://downtownpittsburgh.com/events/\",\n",
    "            \"title\": \"pittsburgh_downtown_events\",\n",
    "            \"date_scraped\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    filename = \"pittsburgh_downtown_events.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {filename} with {len(event_text)} events.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU Events Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "import json\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "base_url = \"https://events.cmu.edu\"\n",
    "url = base_url + \"/all\"\n",
    "driver.get(url)\n",
    "\n",
    "# Click \"Show 50 more\" button until all events are loaded\n",
    "while True:\n",
    "    try:\n",
    "        show_more = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.lw_cal_next\"))\n",
    "        )\n",
    "        show_more.click()\n",
    "        time.sleep(2)\n",
    "    except Exception:\n",
    "        print(\"No more 'Show 50 more' button found or error.\")\n",
    "        break\n",
    "\n",
    "content_output = \"\"\n",
    "events_container = driver.find_element(By.ID, \"lw_cal_events\")\n",
    "children = events_container.find_elements(By.XPATH, \"./*\")\n",
    "current_date = None\n",
    "\n",
    "for child in children:\n",
    "    tag = child.tag_name.lower()\n",
    "\n",
    "    if tag == \"h3\":\n",
    "        if current_date:\n",
    "            content_output += \" ##### \"\n",
    "        current_date = child.text.strip()\n",
    "        content_output += f\"{current_date}: \"\n",
    "\n",
    "    elif tag == \"div\":\n",
    "        try:\n",
    "            event_list = child.find_element(By.CLASS_NAME, \"lw_cal_event_list\")\n",
    "            events = event_list.find_elements(By.CLASS_NAME, \"lw_cal_event_info\")\n",
    "\n",
    "            for event in events:\n",
    "                try:\n",
    "                    title_elem = event.find_element(By.CSS_SELECTOR, \"div.lw_events_title a\")\n",
    "                    event_title = title_elem.text.strip()\n",
    "                    event_href = urljoin(base_url, title_elem.get_attribute(\"href\"))\n",
    "\n",
    "                    try:\n",
    "                        event_location = event.find_element(By.CLASS_NAME, \"lw_events_location\").text.strip()\n",
    "                    except Exception:\n",
    "                        event_location = \"Location: N/A\"\n",
    "\n",
    "                    try:\n",
    "                        event_time = event.find_element(By.CLASS_NAME, \"lw_events_time\").text.strip()\n",
    "                    except Exception:\n",
    "                        event_time = \"Time: N/A\"\n",
    "\n",
    "                    try:\n",
    "                        event_summary = event.find_element(By.CLASS_NAME, \"lw_events_summary\").text.strip()\n",
    "                    except Exception:\n",
    "                        event_summary = \"No summary available.\"\n",
    "\n",
    "                    content_output += f\"{event_title}, {event_time}, {event_location}, {event_summary}, {event_href} || \"\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing event: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing event list for date {current_date}: {e}\")\n",
    "            continue\n",
    "data = {\n",
    "    \"content\": content_output.strip(),\n",
    "    \"metadata\": {\n",
    "        \"source\": \"https://events.cmu.edu/all\",\n",
    "        \"title\": \"Carnegie Mellon University Events\",\n",
    "        \"date_scraped\": datetime.now().isoformat(),\n",
    "        \"depth\": 2,\n",
    "        \"id\": 1\n",
    "    }\n",
    "}\n",
    "with open(\"cmu_all_events.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Scraping complete. Data saved to cmu_all_events.json\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU Drama Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ics import Calendar\n",
    "\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        text = re.sub(r'\\s+', ' ', text)  \n",
    "        return text.strip()  \n",
    "    return \"No Description\"\n",
    "\n",
    "ics_file_path = r\"C:\\Users\\maitr\\OneDrive\\Desktop\\cmu-2025\\anlp\\assignments\\assignment2-pitt-rag-local-copy\\scrapper\\cmu-drama-8e20d4b7c6d.ics\"  # Replace with your file path\n",
    "with open(ics_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    calendar = Calendar(file.read())\n",
    "\n",
    "content_output = \"\"\n",
    "\n",
    "for event in calendar.events:\n",
    "    event_date = event.begin.date().isoformat()  \n",
    "    event_title = clean_text(event.name) if event.name else \"No Title\"\n",
    "    event_start = event.begin.to('local').strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    event_end = event.end.to('local').strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    event_location = clean_text(event.location) if event.location else \"Location: N/A\"\n",
    "    event_description = clean_text(event.description) if event.description else \"No Description\"\n",
    "\n",
    "    content_output += f\"{event_date}: {event_title}, {event_start} - {event_end}, {event_location}, {event_description} || \"\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"content\": content_output.strip(),\n",
    "    \"metadata\": {\n",
    "        \"source\": \"events.ics\",\n",
    "        \"title\": \"Extracted Events\",\n",
    "        \"date_scraped\": datetime.now().isoformat(),\n",
    "        \"depth\": 1,\n",
    "        \"id\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "json_file_path = \"cmu_drama_calendar.json\"\n",
    "with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Conversion complete! Data saved to {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU DRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://drama.cmu.edu/\"\n",
    "MAX_DEPTH = 7  \n",
    "OUTPUT_DIR = \"cmu_drama\"  \n",
    "visited_urls = set()  \n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def extract_text(soup):\n",
    "    elements = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"li\", \"span\", \"div\"])\n",
    "    return clean_text(\"\\n\".join([elem.get_text(strip=True) for elem in elements if elem.get_text(strip=True)]))\n",
    "\n",
    "def generate_filename(url):\n",
    "    parsed = urlparse(url)\n",
    "    slug = parsed.path.strip(\"/\").replace(\"/\", \"_\").replace(\"?\", \"_\").replace(\"=\", \"_\")\n",
    "    return f\"cmu_drama_{slug}.json\" if slug else \"cmu_drama_home.json\"\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✔ Saved: {filepath}\")\n",
    "\n",
    "def crawl(url, depth=1):\n",
    "    if depth > MAX_DEPTH or url in visited_urls:\n",
    "        return \n",
    "    visited_urls.add(url)  \n",
    "    print(f\"Crawling ({depth}): {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  \n",
    "    except requests.RequestException as e:\n",
    "        print(f\" Error fetching {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    title = soup.title.string.strip() if soup.title else \"No Title\"\n",
    "    page_text = extract_text(soup)  \n",
    "    data = {\n",
    "        \"content\": page_text,\n",
    "        \"metadata\": {\n",
    "            \"title\": title,\n",
    "            \"source\": url,\n",
    "            \"date_scraped\": datetime.now().isoformat(),\n",
    "            \"depth\": depth\n",
    "        }\n",
    "    }\n",
    "    save_to_json(data, generate_filename(url))  \n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    for link in links:\n",
    "        full_url = urljoin(BASE_URL, link[\"href\"])  \n",
    "        if urlparse(full_url).netloc == urlparse(BASE_URL).netloc:  \n",
    "            crawl(full_url, depth + 1)\n",
    "\n",
    "crawl(BASE_URL)\n",
    "\n",
    "print(f\"\\nScraping complete! {len(visited_urls)} pages saved in '{OUTPUT_DIR}/'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
