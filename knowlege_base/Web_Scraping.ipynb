{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to mimic a real browser request\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "BASE_URLS = {\n",
    "    \"https://www.visitpittsburgh.com/things-to-do/\":{'category': 'Pittsburgh', 'subcategory':'Events'},\n",
    "}\n",
    "\n",
    "# Output files\n",
    "DATA_FILE = \"output.txt\"\n",
    "ERROR_LOG = \"scraping_errors.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visited URLs (to prevent duplicate scraping)\n",
    "visited_urls = set()\n",
    "\n",
    "# Function to scrape a single webpage\n",
    "def scrape_page(url, depth, max_depth, category, subcategory):\n",
    "    \"\"\"Scrapes headings and paragraphs from a given webpage.\"\"\"\n",
    "    if url in visited_urls or depth > max_depth:\n",
    "        return None  # Avoid re-scraping and exceeding depth\n",
    "\n",
    "    response = requests.get(url, headers=HEADERS, timeout=10)  # 10s timeout\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Skipping {url} (Status Code: {response.status_code})\")\n",
    "        with open(ERROR_LOG, \"a\", encoding=\"utf-8\") as err_file:\n",
    "            err_file.write(f\"Failed: {url} (Status Code: {response.status_code})\\n\")\n",
    "        return None\n",
    "\n",
    "    visited_urls.add(url)  # Mark as visited\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    title_tag = soup.find(\"title\")\n",
    "    title = title_tag.text.strip() if title_tag else \"No Title\"\n",
    "\n",
    "    content_list = [f\"Title: {title}\\nURL: {url}\\n\"]\n",
    "\n",
    "    # Extract headings & paragraphs\n",
    "    for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"]):\n",
    "        if tag.name in [\"h1\", \"h2\", \"h3\"]:  # Headings\n",
    "            content_list.append(f\"\\n{tag.text.strip()}\\n\" + \"-\" * len(tag.text.strip()) + \"\\n\")\n",
    "        elif tag.name == \"p\":  # Paragraphs\n",
    "            text = tag.get_text().strip()\n",
    "            if text:\n",
    "                content_list.append(text)\n",
    "                \n",
    "    content = \"\\n\".join(content_list)\\\n",
    "\n",
    "    max_id = max(\n",
    "        [0] + [\n",
    "            int(s.split(\"_\")[0])\n",
    "            for s in os.listdir(os.getcwd())\n",
    "            if s.endswith('.json')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "    document = {\n",
    "            \"content\": content,\n",
    "            \"metadata\": {\n",
    "                \"source\": url,\n",
    "                \"title\": title,\n",
    "                \"category\": category,\n",
    "                \"subcategory\": subcategory,\n",
    "                \"date_scraped\": datetime.datetime.now().isoformat(),\n",
    "                \"depth\": depth,\n",
    "                'id':max_id + 1\n",
    "            }\n",
    "        }\n",
    "    doc_id = document[\"metadata\"][\"id\"]\n",
    "    filename = f\"{doc_id}_{title}.json\"\n",
    "    # filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(document, f, ensure_ascii=False, indent=2)\n",
    "    return soup\n",
    "\n",
    "# Recursive function to scrape links up to depth 5\n",
    "def recursive_scrape(url, depth, category, subcategory, max_depth = 3, base_url=None):\n",
    "    \"\"\"Recursively scrapes a webpage and follows links up to depth 5.\"\"\"\n",
    "    if depth > max_depth or url in visited_urls:\n",
    "        return\n",
    "    \n",
    "    if not base_url:\n",
    "        base_url = url\n",
    "\n",
    "    print(f\"Scraping (Depth {depth}): {url}\")\n",
    "    soup = scrape_page(url, depth, max_depth, category, subcategory)\n",
    "\n",
    "    if not soup:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    links = []\n",
    "\n",
    "    # Extract all links and filter only CMU internal links\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "        if href.startswith(\"/\"):  # Internal relative link\n",
    "            full_url = base_url.rstrip('/') + href\n",
    "            links.append(full_url)\n",
    "        elif href.startswith((\"http://\", \"https://\")):\n",
    "            # Extract domain from base_url\n",
    "            base_domain = re.search(r'https?://([^/]+)', base_url)\n",
    "            if base_domain and base_domain.group(1) in href:\n",
    "                links.append(href)\n",
    "    for link in links:\n",
    "        if link not in visited_urls:\n",
    "            time.sleep(2)\n",
    "            recursive_scrape(link, depth+1, category, subcategory, max_depth) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_URLS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[43mBASE_URLS\u001b[49m:\n\u001b[0;32m      2\u001b[0m     recursive_scrape(url, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, category \u001b[38;5;241m=\u001b[39m BASE_URLS[url][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m], subcategory \u001b[38;5;241m=\u001b[39mBASE_URLS[url][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubcategory\u001b[39m\u001b[38;5;124m'\u001b[39m],  max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, base_url\u001b[38;5;241m=\u001b[39murl)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BASE_URLS' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for url in BASE_URLS:\n",
    "    recursive_scrape(url, depth=0, category = BASE_URLS[url]['category'], subcategory =BASE_URLS[url]['subcategory'],  max_depth=5, base_url=url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
