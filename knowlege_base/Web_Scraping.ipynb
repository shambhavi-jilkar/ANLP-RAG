{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to mimic a real browser request\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "BASE_URLS = {\n",
    "    # \"https://www.visitpittsburgh.com/things-to-do/\":{'category': 'Pittsburgh', 'subcategory':'Events'},\n",
    "    # \"https://pittsburghrestaurantweek.com/\": {'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.pghtacofest.com/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://littleitalydays.com/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.picklesburgh.com/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},   \n",
    "    # \"https://pittsburghsymphony.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.pghtech.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.thefrickpittsburgh.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/blog/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://trustarts.org/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/events-festivals/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/hotels-resorts/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    # \"https://www.visitpittsburgh.com/restaurants-culinary/\":{'category': 'Pittsburgh', 'subcategory': 'Events'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Cork_Run_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Fort_Pitt_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Liberty_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Mount_Washington_Transit_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Squirrel_Hill_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/Schenley_Tunnel\":{'category': 'Pittsburgh', 'subcategory': 'Facts'},\n",
    "    #  \"https://en.wikipedia.org/wiki/List_of_bridges_of_Pittsburgh\":{'category': 'Pittsburgh', 'subcategory': 'Facts'}\n",
    "    \"https://carnegiemuseums.org/events/\":{'category': 'Pittsburgh', 'subcategory': 'Events'}\n",
    "    \n",
    "}\n",
    "\n",
    "# Output files\n",
    "OUTPUT_DIRECTORY = os.path.join(os.getcwd(), \"json_documents\")\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok = True)\n",
    "ERROR_LOG = \"scraping_errors.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carnegie_museum_events(soup):\n",
    "    event_cards = soup.find_all('article', class_='event-card')\n",
    "    \n",
    "    content_list = []\n",
    "\n",
    "    for card in event_cards:\n",
    "        try:\n",
    "            # Extract event start and end dates\n",
    "            start_date = card.get('data-event-start')\n",
    "            end_date = card.get('data-event-end')\n",
    "            \n",
    "            # Extract recurrence information\n",
    "            recurrence_event = card.get('data-recurring-event', False)\n",
    "            recurrence_frequency = card.get('data-recurring-frequency', None)\n",
    "            recurrence_day_index = card.get('data-day-index', None)\n",
    "            weekday = card.get('data-day-weekday', None)\n",
    "            \n",
    "            # Extract event title and link\n",
    "            title_element = card.find('div', class_='event-card__content')\n",
    "            title = title_element.text.strip() if title_element else \"No title\"\n",
    "            link = None\n",
    "            if title_element and title_element.find('a'):\n",
    "                link = title_element.find('a').get('href')\n",
    "            \n",
    "            # Extract venue information\n",
    "            venue_element = card.find('div', class_='event-card__venue')\n",
    "            venue = None\n",
    "            venue_link = None\n",
    "            if venue_element and venue_element.find('a'):\n",
    "                venue = venue_element.find('a').text.strip()\n",
    "                venue_link = venue_element.find('a').get('href')\n",
    "            \n",
    "            # Extract details\n",
    "            details_element = card.find('div', class_='event-card__details')\n",
    "            details = details_element.text.strip() if details_element else None\n",
    "            \n",
    "            content = f\"{title} starts on {start_date} and ends on {end_date}. It will be located at {venue}. {details}\"\n",
    "            \n",
    "            if recurrence_event:\n",
    "                content += f\"The event occurs on {recurrence_frequency} on {weekday}\"\n",
    "            \n",
    "            content_list.append(content)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing event card: {e}\")\n",
    "            continue\n",
    "        \n",
    "        return \"\\n\".join(content_list)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_base_domain(url):\n",
    "#     pattern = r'^(?:https?:\\/\\/)?(?:www\\.)?([^\\/]+?)\\.(?:com|org|gov|edu)(?:\\/|$)'\n",
    "#     match = re.search(pattern, url)\n",
    "#     return match.group(1) if match else None\n",
    "\n",
    "# Function to scrape a single webpage\n",
    "def scrape_page(url, depth, max_depth, category, subcategory, scraped_urls):\n",
    "    \"\"\"Scrapes headings and paragraphs from a given webpage.\"\"\"\n",
    "    if depth > max_depth:\n",
    "        return None \n",
    "\n",
    "    response = requests.get(url, headers=HEADERS, timeout=10)  # 10s timeout\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Skipping {url} (Status Code: {response.status_code})\")\n",
    "        with open(ERROR_LOG, \"a\", encoding=\"utf-8\") as err_file:\n",
    "            err_file.write(f\"Failed: {url} (Status Code: {response.status_code})\\n\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    if url not in scraped_urls:\n",
    "        print(f\"Scraping (Depth {depth}): {url}\")\n",
    "        title_tag = soup.find(\"title\")\n",
    "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
    "        title = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1F]', '', title)\n",
    "\n",
    "        content_list = [f\"Title: {title}\\nURL: {url}\\n\"]\n",
    "\n",
    "        # Extract headings & paragraphs\n",
    "        for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\"]):\n",
    "            if tag.name in [\"h1\", \"h2\", \"h3\"]:  # Headings\n",
    "                content_list.append(f\"\\n{tag.text.strip()}\\n\" + \"-\" * len(tag.text.strip()) + \"\\n\")\n",
    "            elif tag.name == \"p\":  # Paragraphs\n",
    "                text = tag.get_text().strip()\n",
    "                if text:\n",
    "                    content_list.append(text)\n",
    "                    content = \"\\n\".join(content_list)\n",
    "                    \n",
    "        if \"https://carnegiemuseums.org/events\" in url:\n",
    "            content = carnegie_museum_events(soup)\n",
    "        \n",
    "        \n",
    "        max_id = max(\n",
    "            [0] + [\n",
    "                int(s.split(\"_\")[0])\n",
    "                for s in os.listdir(OUTPUT_DIRECTORY)\n",
    "                if s.endswith('.json')\n",
    "            ]\n",
    "        )\n",
    "        document = {\n",
    "                \"content\": content,\n",
    "                \"metadata\": {\n",
    "                    \"source\": url,\n",
    "                    \"title\": title,\n",
    "                    \"category\": category,\n",
    "                    \"subcategory\": subcategory,\n",
    "                    \"date_scraped\": datetime.datetime.now().isoformat(),\n",
    "                    \"depth\": depth,\n",
    "                    'id':max_id + 1\n",
    "                }\n",
    "            }\n",
    "        doc_id = document[\"metadata\"][\"id\"]\n",
    "        \n",
    "        filename = f\"{doc_id}_{title}.json\"\n",
    "        filepath = os.path.join(OUTPUT_DIRECTORY, filename)\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(document, f, ensure_ascii=False, indent=2)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def recursive_scrape(url, depth, category, subcategory, visited_urls, scraped_urls, max_depth = 3, base_url=None):\n",
    "    if depth > max_depth or url == \"https://carnegiemuseums.org/\" or url == \"https://carnegiemuseums.org\":\n",
    "        return\n",
    "    \n",
    "    if not base_url:\n",
    "        # parsed_url = urllib.parse.urlparse(url)\n",
    "        # base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        base_url = url\n",
    "    \n",
    "    visited_urls.add(url)\n",
    "    \n",
    "    soup = None\n",
    "    soup = scrape_page(url, depth, max_depth, category, subcategory, scraped_urls)\n",
    "    scraped_urls.add(url)\n",
    "    if not soup:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    if max_depth > 0 and depth < max_depth:\n",
    "        links = []\n",
    "        \n",
    "        if \"en.wikipedia\" in url:\n",
    "            tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "            links = []\n",
    "            for table in tables:\n",
    "                for row in table.find_all(\"tr\")[1:]: \n",
    "                    if row and len(row) > 0:\n",
    "                        link_tags = row.find_all(\"a\", href=True)\n",
    "                        if link_tags and link_tags[0][\"href\"].startswith(\"/wiki/\"):\n",
    "                            link = link_tags[0]\n",
    "                            links.append(\"https://en.wikipedia.org\" + link[\"href\"])\n",
    "            \n",
    "\n",
    "        else:\n",
    "            full_url = None\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                href = link[\"href\"]\n",
    "                full_url = None\n",
    "\n",
    "                if href.startswith(\"/\"):\n",
    "                    full_url = urllib.parse.urljoin(base_url, href)  # Use urljoin to correctly resolve relative paths\n",
    "                elif href.startswith((\"http://\", \"https://\")):\n",
    "                    parsed_href = urllib.parse.urlparse(href)\n",
    "                    parsed_base = urllib.parse.urlparse(base_url)\n",
    "\n",
    "                    # Ensure the link belongs to the same domain\n",
    "                    if parsed_href.netloc == parsed_base.netloc:\n",
    "                        full_url = href\n",
    "\n",
    "                if full_url and full_url not in visited_urls:\n",
    "                    links.append(full_url)\n",
    "        \n",
    "        print(f\"Found {len(links)} links at {url}\")\n",
    "        # print(links)\n",
    "        for link in links:\n",
    "            if link not in visited_urls:\n",
    "                time.sleep(2)\n",
    "                recursive_scrape(link, depth+1, category, subcategory, visited_urls, scraped_urls, max_depth) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scraped_urls(documents_dir):\n",
    "    scraped_urls = set()\n",
    "    \n",
    "    for filename in os.listdir(documents_dir):\n",
    "        if not filename.endswith('.json'):\n",
    "            continue\n",
    "        filepath = os.path.join(documents_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            document = json.load(f)\n",
    "            if 'source' in document['metadata']:\n",
    "                url = document['metadata']['source']\n",
    "                scraped_urls.add(url)\n",
    "    \n",
    "    return scraped_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 links at https://carnegiemuseums.org/events/\n",
      "Found 111 links at https://carnegiemuseums.org/join-support/membership/joinrenew/\n",
      "Found 109 links at https://carnegiemuseums.org/timed-tickets/\n",
      "Scraping (Depth 1): https://carnegiemuseums.org/events?museum=carnegie-museum-of-art\n",
      "Found 16 links at https://carnegiemuseums.org/events?museum=carnegie-museum-of-art\n",
      "Scraping (Depth 2): https://carnegiemuseums.org/events?event_type=exhibitions\n",
      "Found 3 links at https://carnegiemuseums.org/events?event_type=exhibitions\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events?museum=carnegie-science-center\n",
      "Scraping (Depth 2): https://carnegiemuseums.org/events?event_type=activities\n",
      "Found 23 links at https://carnegiemuseums.org/events?event_type=activities\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events?museum=carnegie-museum-of-natural-history\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events?museum=the-andy-warhol-museum\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events?audience=just-for-teens\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events?audience=kids-family\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events/page/2/?event_type=activities\n",
      "Scraping (Depth 2): https://carnegiemuseums.org/events/page/2/?museum=carnegie-museum-of-art\n",
      "Found 3 links at https://carnegiemuseums.org/events/page/2/?museum=carnegie-museum-of-art\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events/?museum=carnegie-museum-of-art\n",
      "Scraping (Depth 1): https://carnegiemuseums.org/events?event_type=after-hours-events\n",
      "Found 2 links at https://carnegiemuseums.org/events?event_type=after-hours-events\n",
      "Scraping (Depth 2): https://carnegiemuseums.org/events?audience=just-for-adults\n",
      "Found 2 links at https://carnegiemuseums.org/events?audience=just-for-adults\n",
      "Scraping (Depth 3): https://carnegiemuseums.org/events?event_type=talks-tours\n",
      "Scraping (Depth 1): https://carnegiemuseums.org/events/page/2/\n",
      "Found 3 links at https://carnegiemuseums.org/events/page/2/\n",
      "Scraping (Depth 2): https://carnegiemuseums.org/events/page/3/\n",
      "Found 1 links at https://carnegiemuseums.org/events/page/3/\n"
     ]
    }
   ],
   "source": [
    "# Visited URLs (to prevent duplicate scraping)\n",
    "scraped_urls = get_scraped_urls(OUTPUT_DIRECTORY)\n",
    "visited_urls = set()\n",
    "\n",
    "for url in BASE_URLS:\n",
    "    recursive_scrape(url, \n",
    "                     depth=0, \n",
    "                     category = BASE_URLS[url]['category'], \n",
    "                     subcategory =BASE_URLS[url]['subcategory'], \n",
    "                     visited_urls = visited_urls, \n",
    "                     scraped_urls=scraped_urls, \n",
    "                     max_depth=3, \n",
    "                     base_url=url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "SKIP_PATTERN = re.compile(r\"_Terms|_Notice|_Become|_Privacy\")\n",
    "\n",
    "for filename in os.listdir(OUTPUT_DIRECTORY):\n",
    "    count = 0\n",
    "    if SKIP_PATTERN.search(filename): \n",
    "        count+=1\n",
    "        os.remove(os.path.join(OUTPUT_DIRECTORY, filename))\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
